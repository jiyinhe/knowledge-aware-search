Scripts used to preprocess the logs

- Each log has a different format and different parculiarity, 
 so a dedicated script is used to format and prepare the log. 

- The final output file is in json format
{'uid': userid (either exact or proximated, or leave as "U")
 'qid': querid that is unique within a single user, for example, composed of uid + timestamp
 'qtext': query text,
 'urls': [(url1, timestamp, rank), (url2 ..], clicked urls
 'qtype': query type: [new|page|na] whether it's a new query or a pagination, or not known
 'qtime': timestamp of the query
 'qel': entity linking result of the query
}


logParser.py: 
	python script to parse the logs and prepare for further processing, e.g. run FEL tool
	It contains 4 stages:
	- phase1: extract info needed for final output
	- phase2: extract queries for FEL
	- phase3: combine results of phase1 and 2
	- phase4: group log by user
run_logparser.sh: 
	to run the logParser script to pre-process the logs
run_el.sh: 
	to run the script to run fast entity linking

WPClick.py: 
	python script to compute stats of WP click data
	It has 3 modes:
	- tot: compute the total number of clicks of "curr" concepts
	- sta: stationary probability of "curr" concept being reached, using tot as a parameter
	- tra: transitional probability from "prev" to "curr" P(curr|prev).
run_wpclicks.sh: 
	to run the WPClick.py with all click data included.
run_wpclicks_perfile.sh: 
	to run the WPClick.py wich individual WP click data dump.




